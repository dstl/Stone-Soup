param_config:
  _AlgorithmConfig__prior_exploration_config:
    type: StochasticSampling
  _disable_action_flattening: false
  _disable_execution_plan_api: true
  _disable_initialize_loss_from_dummy_batch: false
  _disable_preprocessor_api: false
  _enable_learner_api: true
  _enable_rl_module_api: true
  _fake_gpus: false
  _is_atari: null
  _learner_class: null
  _tf_policy_handles_more_than_one_loss: false
  action_mask_key: action_mask
  action_space: null
  actions_in_input_normalized: false
  algorithm_config_overrides_per_module: {}
  always_attach_evaluation_results: false
  auto_wrap_old_gym_envs: true
  batch_mode: complete_episodes
  callbacks: !!python/name:ray.rllib.algorithms.callbacks.DefaultCallbacks ''
  checkpoint_trainable_policies_only: false
  clip_actions: false
  clip_param: 0.3
  clip_rewards: null
  compress_observations: false
  count_steps_by: env_steps
  create_env_on_driver: false
  custom_eval_function: null
  custom_resources_per_worker: {}
  delay_between_worker_restarts_s: 60.0
  disable_env_checking: false
  eager_max_retraces: 20
  eager_tracing: true
  enable_async_evaluation: false
  enable_connectors: true
  enable_tf1_exec_eagerly: false
  entropy_coeff: 0.0
  entropy_coeff_schedule: null
  env: StoneSoupEnv-v1
  env_config: {}
  env_runner_cls: null
  env_task_fn: null
  evaluation_config: null
  evaluation_duration: 10
  evaluation_duration_unit: episodes
  evaluation_interval: null
  evaluation_num_workers: 0
  evaluation_parallel_to_training: false
  evaluation_sample_timeout_s: 180.0
  exploration_config: {}
  explore: true
  export_native_model_files: false
  extra_python_environs_for_driver: {}
  extra_python_environs_for_worker: {}
  fake_sampler: false
  framework: torch
  gamma: 0.99
  grad_clip: null
  grad_clip_by: global_norm
  ignore_worker_failures: false
  in_evaluation: false
  input: sampler
  input_config: {}
  keep_per_episode_custom_metrics: false
  kl_coeff: 0.2
  kl_target: 0.01
  lambda: 1.0
  local_gpu_idx: 0
  local_tf_session_args:
    inter_op_parallelism_threads: 8
    intra_op_parallelism_threads: 8
  log_level: WARN
  log_sys_usage: true
  logger_config: null
  logger_creator: null
  lr: 0.0001
  lr_schedule: null
  max_num_worker_restarts: 1000
  max_requests_in_flight_per_sampler_worker: 2
  metrics_episode_collection_timeout_s: 60.0
  metrics_num_episodes_for_smoothing: 100
  min_sample_timesteps_per_iteration: 0
  min_time_s_per_iteration: null
  min_train_timesteps_per_iteration: 0
  model:
    _disable_action_flattening: false
    _disable_preprocessor_api: false
    _time_major: false
    _use_default_native_models: -1
    always_check_shapes: false
    attention_dim: 64
    attention_head_dim: 32
    attention_init_gru_gate_bias: 2.0
    attention_memory_inference: 50
    attention_memory_training: 50
    attention_num_heads: 1
    attention_num_transformer_units: 1
    attention_position_wise_mlp_dim: 32
    attention_use_n_prev_actions: 0
    attention_use_n_prev_rewards: 0
    conv_activation: relu
    conv_filters: null
    custom_action_dist: null
    custom_model: null
    custom_model_config: {}
    custom_preprocessor: null
    dim: 84
    encoder_latent_dim: null
    fcnet_activation: tanh
    fcnet_hiddens:
    - 256
    - 256
    framestack: true
    free_log_std: false
    grayscale: false
    lstm_cell_size: 256
    lstm_use_prev_action: false
    lstm_use_prev_action_reward: -1
    lstm_use_prev_reward: false
    max_seq_len: 20
    no_final_linear: false
    post_fcnet_activation: relu
    post_fcnet_hiddens: []
    use_attention: false
    use_lstm: false
    vf_share_layers: false
    zero_mean: true
  normalize_actions: true
  num_consecutive_worker_failures_tolerance: 100
  num_cpus_for_driver: 1
  num_cpus_per_learner_worker: 1
  num_cpus_per_worker: 1
  num_envs_per_worker: 1
  num_gpus: 0
  num_gpus_per_learner_worker: 0
  num_gpus_per_worker: 0
  num_learner_workers: 0
  num_sgd_iter: 30
  num_workers: 1
  observation_filter: NoFilter
  observation_fn: null
  observation_space: null
  off_policy_estimation_methods: {}
  offline_sampling: false
  ope_split_batch_by_episode: true
  optimizer: {}
  output: null
  output_compress_columns:
  - obs
  - new_obs
  output_config: {}
  output_max_file_size: 67108864
  placement_strategy: PACK
  policies:
    default_policy: !!python/tuple
    - null
    - null
    - null
    - null
  policies_to_train: null
  policy_map_cache: -1
  policy_map_capacity: 100
  policy_mapping_fn: default_policy
  policy_states_are_swappable: false
  postprocess_inputs: false
  preprocessor_pref: deepmind
  recreate_failed_workers: false
  remote_env_batch_wait_ms: 0
  remote_worker_envs: false
  render_env: false
  replay_sequence_length: null
  restart_failed_sub_environments: false
  rl_module_spec: null
  rollout_fragment_length: auto
  sample_async: false
  sample_collector: !!python/name:ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector ''
  sampler_perf_stats_ema_coef: null
  seed: null
  sgd_minibatch_size: 128
  shuffle_buffer_size: 0
  shuffle_sequences: true
  simple_optimizer: -1
  sync_filters_on_rollout_workers_timeout_s: 60.0
  synchronize_filters: -1
  tf_session_args:
    allow_soft_placement: true
    device_count:
      CPU: 1
    gpu_options:
      allow_growth: true
    inter_op_parallelism_threads: 2
    intra_op_parallelism_threads: 2
    log_device_placement: false
  torch_compile_learner: false
  torch_compile_learner_dynamo_backend: inductor
  torch_compile_learner_dynamo_mode: null
  torch_compile_learner_what_to_compile: forward_train
  torch_compile_worker: false
  torch_compile_worker_dynamo_backend: onnxrt
  torch_compile_worker_dynamo_mode: null
  train_batch_size: 4000
  update_worker_filter_stats: true
  use_critic: true
  use_gae: true
  use_kl_loss: true
  use_worker_filter_stats: true
  validate_workers_after_construction: true
  vf_clip_param: 10.0
  vf_loss_coeff: 1.0
  vf_share_layers: -1
  worker_cls: -1
  worker_health_probe_timeout_s: 60
  worker_restore_timeout_s: 1800
run_config:
  callbacks: null
  checkpoint_config:
    checkpoint_at_end: true
    checkpoint_frequency: 3
    checkpoint_score_attribute: episode_reward_mean
    num_to_keep: 3
  name: PPO
  stop:
    timesteps_total: 10000000
  verbose: 1
trainable: PPO