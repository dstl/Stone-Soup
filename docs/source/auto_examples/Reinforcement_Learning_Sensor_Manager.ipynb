{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Reinforcement Learning Sensor Manager\nThis example looks at how to interface a reinforcement learning framework with a Stone Soup sensor manager.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making a Reinforcement Learning Sensor Manager\nThis example introduces using a Deep Q Network (DQN) reinforcement learning (RL) sensor management algorithm\nin Stone Soup.\n\nThis is compared to the performance of a brute force algorithm using the same metrics shown in the sensor\nmanagement tutorials.\n\nThis example is similar to the tutorials, simulating 3 targets and a :class:`~.RadarRotatingBearingRange` sensor,\nwhich can be actioned to point in different directions.\n\nTensorflow-agents is used as the reinforcement learning framework. This is a separate python package that can be found\nat https://github.com/tensorflow/agents. This currently only works on Linux based OSes, or via Windows Subsystem for\nLinux (WSL). See Tensorflow instructions for creating environments (with GPU support if applicable) [#]_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Some general imports and set up\nimport numpy as np\nimport random\nfrom datetime import datetime, timedelta\n\nstart_time = datetime.now().replace(microsecond=0)\n\nfrom stonesoup.models.transition.linear import CombinedLinearGaussianTransitionModel, ConstantVelocity\nfrom stonesoup.types.groundtruth import GroundTruthPath, GroundTruthState"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate ground truths\nFollowing the methods from previous Stone Soup tutorials, generate a series of combined linear Gaussian transition\nmodels and generate ground truths. Each ground truth is offset in the y-direction by 10.\n\nThe number of targets in this simulation is defined by `ntruths` - here there are 3 targets travelling in different\ndirections. The time the simulation is observed for is defined by `time_max`.\n\nWe can fix our random number generator in order to probe a particular example repeatedly. To produce random examples,\ncomment out the next two lines.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(1990)\nrandom.seed(1990)\n\n# Generate transition model\n# i.e. fk(xk|xk-1)\ntransition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(0.005),\n                                                          ConstantVelocity(0.005)])\n\nyps = range(0, 100, 10)  # y value for prior state\ntruths = []\nntruths = 3  # number of ground truths in simulation\ntime_max = 50  # timestamps the simulation is observed over\ntimesteps = [start_time + timedelta(seconds=k) for k in range(time_max)]\n\nxdirection = 1\nydirection = 1\n\n# Generate ground truths\nfor j in range(0, ntruths):\n    truth = GroundTruthPath([GroundTruthState([0, xdirection, yps[j], ydirection],\n                                              timestamp=start_time)],\n                            id=f\"id{j}\")\n\n    for k in range(1, time_max):\n        truth.append(\n            GroundTruthState(transition_model.function(truth[k - 1], noise=True, time_interval=timedelta(seconds=1)),\n                             timestamp=start_time + timedelta(seconds=k)))\n    truths.append(truth)\n\n    # alternate directions when initiating tracks\n    xdirection *= -1\n    if j % 2 == 0:\n        ydirection *= -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the ground truths. This is done using the :class:`~.Plotterly` class from Stone Soup.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.plotter import AnimatedPlotterly\n\nplotter = AnimatedPlotterly(timesteps, tail_length=1)\nplotter.plot_ground_truths(truths, [0, 2])\nplotter.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create sensors\nCreate a sensor for each sensor management algorithm. This tutorial uses the\n:class:`~.RadarRotatingBearingRange` sensor. This sensor is an :class:`~.Actionable` so\nis capable of returning the actions it can possibly\ntake at a given time step and can also be given an action to take before taking\nmeasurements.\nSee the Creating an Actionable Sensor Example for a more detailed explanation of actionable sensors.\n\nThe :class:`~.RadarRotatingBearingRange` has a dwell centre which is an :class:`~.ActionableProperty`\nso in this case the action is changing the dwell centre to point in a specific direction.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.types.state import StateVector\nfrom stonesoup.sensor.radar.radar import RadarRotatingBearingRange\n\nsensorA = RadarRotatingBearingRange(\n    position_mapping=(0, 2),\n    noise_covar=np.array([[np.radians(0.5) ** 2, 0],\n                          [0, 1 ** 2]]),\n    ndim_state=4,\n    position=np.array([[10], [0]]),\n    rpm=60,\n    fov_angle=np.radians(45),\n    dwell_centre=StateVector([0.0]),\n    max_range=np.inf\n)\nsensorA.timestamp = start_time\n\nsensorB = RadarRotatingBearingRange(\n    position_mapping=(0, 2),\n    noise_covar=np.array([[np.radians(0.5) ** 2, 0],\n                          [0, 1 ** 2]]),\n    ndim_state=4,\n    position=np.array([[10], [0]]),\n    rpm=60,\n    fov_angle=np.radians(45),\n    dwell_centre=StateVector([0.0]),\n    max_range=np.inf\n)\nsensorB.timestamp = start_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the Kalman predictor and updater\nConstruct a predictor and updater using the :class:`~.KalmanPredictor` and :class:`~.ExtendedKalmanUpdater`\ncomponents from Stone Soup. The :class:`~.ExtendedKalmanUpdater` is used because it can be used for both linear\nand nonlinear measurement models.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.predictor.kalman import KalmanPredictor\npredictor = KalmanPredictor(transition_model)\n\nfrom stonesoup.updater.kalman import ExtendedKalmanUpdater\nupdater = ExtendedKalmanUpdater(measurement_model=None)\n# measurement model is added to detections by the sensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the Kalman filters\nFirst create `ntruths` priors which estimate the targets\u2019 initial states, one for each target. In this example\neach prior is offset by 0.5 in the y direction meaning the position of the track is initially not very accurate. The\nvelocity is also systematically offset by +0.5 in both the x and y directions.\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.types.state import GaussianState\n\npriors = []\nxdirection = 1.2\nydirection = 1.2\nfor j in range(0, ntruths):\n    priors.append(GaussianState([[0], [xdirection], [yps[j]+0.1], [ydirection]],\n                                np.diag([0.5, 0.5, 0.5, 0.5]+np.random.normal(0,5e-4,4)),\n                                timestamp=start_time))\n    xdirection *= -1\n    if j % 2 == 0:\n        ydirection *= -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialise the tracks by creating an empty list and appending the priors generated. This needs to be done separately\nfor both sensor manager methods as they will generate different sets of tracks.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.types.track import Track\n\n# Initialise tracks from the RandomSensorManager\ntracksA = []\nfor j, prior in enumerate(priors):\n    tracksA.append(Track([prior]))\n\ntracksB = []\nfor j, prior in enumerate(priors):\n    tracksB.append(Track([prior]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reward function\nA reward function is used to quantify the benefit of sensors taking a particular action or set of actions.\nThis can be crafted specifically for an example in order to achieve a particular objective. The function used in\nthis example is quite generic but could be substituted for any callable function which returns a numeric\nvalue that the sensor manager can maximise.\n\nThe :class:`~.UncertaintyRewardFunction` calculates the uncertainty reduction by computing the difference between the\ncovariance matrix norms of the prediction, and the posterior assuming a predicted measurement corresponding to that\nprediction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.hypothesiser.distance import DistanceHypothesiser\nfrom stonesoup.measures import Mahalanobis\nhypothesiser = DistanceHypothesiser(predictor, updater, measure=Mahalanobis(), missed_distance=5)\n\nfrom stonesoup.dataassociator.neighbour import GNNWith2DAssignment\ndata_associator = GNNWith2DAssignment(hypothesiser)\n\nfrom stonesoup.sensormanager.reward import UncertaintyRewardFunction\nreward_function = UncertaintyRewardFunction(predictor=predictor, updater=updater)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reinforcement Learning\nReinforcement learning is when an intelligent agents make decisions to maximise a cumulative reward. First, the agent\nmust train in an environment to create a policy, which determines which actions to take. During training, the agent\nmakes decisions and receives rewards, which is uses to optimise the policy.\n\n<img src=\"https://stonesoup.rtfd.io/en/latest/_static/rl_training.png\" width=\"800\" alt=\"Illustration of sequential actions and measurements\">\n\nOnce training has completed, the policy can be exploited to gain rewards.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Design Environment\nAn environment is needed for the RL agent to learn in. There are resources online for how to design these [#]_.\n\nFor this tutorial, a pre-designed environment has been created for you to go through.\nIn this example, the action space is equal to the number of targets in the simulation, so at each time step, the\nsensor can select one target to look at.\nThe :class:`~.UncertaintyRewardFunction` is used to calculate the reward obtained for each step in the environment.\nThe trace of the covariances for each object is used as the observation for the agent to learn from - it should learn\nto select targets with a larger covariance (higher uncertainty).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from abc import ABC\nimport numpy as np\nimport copy\nfrom ordered_set import OrderedSet\n\nfrom stonesoup.sensor.action.dwell_action import DwellActionsGenerator\nfrom stonesoup.functions import mod_bearing\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.environments import utils\n\n\nclass StoneSoupEnv(py_environment.PyEnvironment, ABC):\n    \"\"\"Example reinforcement learning environment. Environments must contain __init__, _reset,\n    _step, and generate_action methods\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # Action size is number of targets\n        self._action_spec = array_spec.BoundedArraySpec(\n            shape=(), dtype=np.int32, minimum=0, maximum=ntruths-1, name='action')\n        # Observation size is also number of targets\n        self.obs_size = ntruths\n        self._observation_spec = array_spec.BoundedArraySpec(\n            shape=(self.obs_size,), dtype=np.float32, name='observation')\n        self._episode_ended = False\n        self.max_episode_length = time_max\n        self.current_step = 0\n        self.start_time = start_time\n        # Use deepcopy to prevent the original sensor/tracks being changed\n        self.sensor = copy.deepcopy(sensorA)\n        self.sensor.timestamp = start_time\n        self.tracks = copy.deepcopy(tracksA)\n        \n    def action_spec(self):\n        \"\"\"Return action_spec.\"\"\"\n        return self._action_spec\n\n    def observation_spec(self):\n        \"\"\"Return observation_spec.\"\"\"\n        return self._observation_spec\n\n    def _reset(self):\n        \"\"\"Restarts the environment from the first step, resets the initial state\n        and observation values, and returns an initial observation\n        \"\"\"\n        self._episode_ended = False\n        self.current_step = 0\n        self.sensor = copy.deepcopy(sensorA)\n        self.sensor.timestamp = start_time\n        self.tracks = copy.deepcopy(tracksA)\n        return ts.restart(np.zeros((self.obs_size,), dtype=np.float32))\n\n    def _step(self, action):\n        \"\"\"Apply action and take one step through environment, and return new time_step.\n        \"\"\"\n\n        reward = 0\n        if self._episode_ended:\n            # The last action ended the episode. Ignore the current action and start\n            # a new episode.\n            return self.reset()\n\n        uncertainty = []\n        for i, target in enumerate(self.tracks):\n            # Calculate the bearing of the chosen target from the sensor\n            if i == action:\n                x_target = target.state.state_vector[0]-self.sensor.position[0]\n                y_target = target.state.state_vector[2]-self.sensor.position[1]\n                bearing_target = mod_bearing(np.arctan2(y_target, x_target))\n\n            uncertainty.append(np.trace(target.covar))\n\n        current_timestep = self.start_time + timedelta(seconds=self.current_step)\n        next_timestep = self.start_time + timedelta(seconds=self.current_step+1)\n\n        # Create action generator which contains possible actions\n        action_generator = DwellActionsGenerator(self.sensor,\n                                                 attribute='dwell_centre',\n                                                 start_time=current_timestep,\n                                                 end_time=next_timestep)\n\n        # Action the environment's sensor to point towards the chosen target\n        current_action = [action_generator.action_from_value(bearing_target)]\n        config = ({self.sensor: current_action})\n        reward += reward_function(config, self.tracks, next_timestep)\n\n        self.sensor.add_actions(current_action)\n        self.sensor.act(next_timestep)\n\n        # Calculate a measurement from the sensor\n        measurement = set()\n        measurement |= self.sensor.measure(OrderedSet(truth[current_timestep] for truth in truths), noise=True)\n        hypotheses = data_associator.associate(self.tracks,\n                                               measurement,\n                                               current_timestep)\n\n        for track in self.tracks:\n            hypothesis = hypotheses[track]\n            if hypothesis.measurement:\n                post = updater.update(hypothesis)\n                track.append(post)\n            else:  # When data associator says no detections are good enough, we'll keep the prediction\n                track.append(hypothesis.prediction)\n\n        # Set the observation as the prior uncertainty of each target\n        observation = np.array(uncertainty, dtype=np.float32)\n\n        self.current_step += 1\n\n        if self.current_step >= self.max_episode_length-1:\n            self._episode_ended = True\n            return ts.termination(observation, reward)\n        else:\n            return ts.transition(observation, reward=reward, discount=1.0)\n\n    @staticmethod\n    def generate_action(action, tracks, sensor):\n        \"\"\"This method is used to convert a tf-agents action into a Stone Soup action\"\"\"\n        for i, target in enumerate(tracks):\n            if i == action:\n                x_target = target.state.state_vector[0]-sensor.position[0]\n                y_target = target.state.state_vector[2]-sensor.position[1]\n                action_bearing = mod_bearing(np.arctan2(y_target, x_target))\n\n        action_generators = DwellActionsGenerator(sensor,\n                                                  attribute='dwell_centre',\n                                                  start_time=sensor.timestamp,\n                                                  end_time=sensor.timestamp+timedelta(seconds=1))\n\n        current_action = [action_generators.action_from_value(action_bearing)]\n        return current_action\n\n# Validate the environment to ensure that the environment returns the expected specs\ntrain_env = StoneSoupEnv()\nutils.validate_py_environment(train_env, episodes=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RL Sensor Manager\nTo be able to use the RL environment we have designed, we need to make a ReinforcementSensorManager class, which\ninherits from :class:`~.SensorManager`.\n\nWe introduce some additional methods that are used by tensorflow-agents: :func:`compute_avg_return`,\n:func:`dense_layer`, and :func:`train`.\n:func:`compute_avg_return` is used to find the average reward by using a given policy. This is used to evaluate the\ntraining.\n:func:`dense_layer` is used when generating the Q-Network, a neural network model that learns to predict Q-Values.\n\n:func:`train` is used to generate the policy by running a large number of episodes through the Q-Network to work out\nwhich actions are best.\n\nWe also need to re-define the :func:`choose_actions` method from :class:`~.SensorManager` to be able to interface\nStone Soup actions with tensorflow-agent actions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.sensormanager.base import SensorManager\nfrom stonesoup.base import Property\nfrom tf_agents.environments import tf_py_environment\n\n\nclass ReinforcementSensorManager(SensorManager):\n    \"\"\"A sensor manager that employs reinforcement learning algorithms from tensorflow-agents.\n    The sensor manager trains on an environment to find an optimal policy, which is then exploited\n    to choose actions.\n    \"\"\"\n    env: py_environment.PyEnvironment = Property(doc=\"The environment which the agent learns the policy with.\")\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.tf_env = tf_py_environment.TFPyEnvironment(self.env)\n        self.test_env = tf_py_environment.TFPyEnvironment(self.env)\n        self.agent = None\n\n    @staticmethod\n    def compute_avg_return(environment, policy, num_episodes=10):\n        \"\"\"Used to calculate the average reward over a set of episodes.\n\n        Parameters\n        ----------\n        environment:\n            tf-agents environment for evaluating policy on\n\n        policy:\n            tf-agents policy for choosing actions in environment\n\n        num_episodes: int\n            Number of episodes to sample over\n\n        Returns\n        -------\n        : int\n            average reward calculated over num_episodes\n\n        \"\"\"\n        time_step = None\n        episode_return = None\n        total_return = 0.0\n        for _ in range(num_episodes):\n            time_step = environment.reset()\n            episode_return = 0.0\n\n            while not time_step.is_last():\n                action_step = policy.action(time_step)\n                time_step = environment.step(action_step.action)\n                episode_return += time_step.reward\n            total_return += episode_return\n\n        avg_return = total_return / num_episodes\n        return avg_return.numpy()[0]\n\n    @staticmethod\n    def dense_layer(num_units):\n        \"\"\"Method for generating fully connected layers for use in the neural network.\n\n        Parameters\n        ----------\n        num_units: int\n            Number of nodes in dense layer\n\n        Returns\n        -------\n        : tensorflow dense layer\n\n        \"\"\"\n        # Define a helper function to create Dense layers configured with the right\n        # activation and kernel initializer.\n        return tf.keras.layers.Dense(\n            num_units,\n            activation=tf.keras.activations.relu,\n            kernel_initializer=tf.keras.initializers.VarianceScaling(\n                scale=2.0, mode='fan_in', distribution='truncated_normal'))\n\n    def train(self, hyper_parameters):\n        \"\"\"Trains a DQN agent on the specified environment to learn a policy that is later\n        used to select actions.\n\n        Parameters\n        ----------\n        hyper_parameters: dict\n            Dictionary containing hyperparameters used in training. See tutorial for\n            necessary hyperparameters.\n\n        \"\"\"\n        if self.env is not None:\n            self.env.reset()\n\n            train_py_env = self.env\n            eval_py_env = self.env\n            self.train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n            self.eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\n            fc_layer_params = hyper_parameters['fc_layer_params']\n            action_tensor_spec = tensor_spec.from_spec(self.env.action_spec())\n            num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n\n            # QNetwork consists of a sequence of Dense layers followed by a dense layer\n            # with `num_actions` units to generate one q_value per available action as\n            # its output.\n\n            dense_layers = [self.dense_layer(num_units) for num_units in fc_layer_params]\n            q_values_layer = tf.keras.layers.Dense(\n                num_actions,\n                activation=None,\n                kernel_initializer=tf.keras.initializers.RandomUniform(\n                    minval=-0.03, maxval=0.03),\n                bias_initializer=tf.keras.initializers.Constant(-0.2))\n            q_net = sequential.Sequential(dense_layers + [q_values_layer])\n\n            optimizer = tf.keras.optimizers.Adam(hyper_parameters['learning_rate'])\n\n            train_step_counter = tf.Variable(0)\n\n            self.agent = dqn_agent.DdqnAgent(\n                self.train_env.time_step_spec(),\n                self.train_env.action_spec(),\n                q_network=q_net,\n                optimizer=optimizer,\n                td_errors_loss_fn=common.element_wise_squared_loss,\n                train_step_counter=train_step_counter)\n\n            self.agent.initialize()\n\n            random_policy = random_tf_policy.RandomTFPolicy(self.train_env.time_step_spec(),\n                                                            self.train_env.action_spec())\n\n            # See also the metrics module for standard implementations of different metrics.\n            # https://github.com/tensorflow/agents/tree/master/tf_agents/metrics\n\n            self.compute_avg_return(self.eval_env, random_policy,\n                                    hyper_parameters['num_eval_episodes'])\n\n            table_name = 'uniform_table'\n            replay_buffer_signature = tensor_spec.from_spec(\n                self.agent.collect_data_spec)\n            replay_buffer_signature = tensor_spec.add_outer_dim(\n                replay_buffer_signature)\n\n            table = reverb.Table(\n                table_name,\n                max_size=hyper_parameters['replay_buffer_max_length'],\n                sampler=reverb.selectors.Uniform(),\n                remover=reverb.selectors.Fifo(),\n                rate_limiter=reverb.rate_limiters.MinSize(1),\n                signature=replay_buffer_signature)\n\n            reverb_server = reverb.Server([table])\n\n            replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n                self.agent.collect_data_spec,\n                table_name=table_name,\n                sequence_length=2,\n                local_server=reverb_server)\n\n            rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n                replay_buffer.py_client,\n                table_name,\n                sequence_length=2)\n\n            py_driver.PyDriver(\n                self.env,\n                py_tf_eager_policy.PyTFEagerPolicy(\n                    random_policy, use_tf_function=True),\n                [rb_observer],\n                max_steps=hyper_parameters['initial_collect_steps']).run(train_py_env.reset())\n\n            # Dataset generates trajectories with shape [Bx2x...]\n            dataset = replay_buffer.as_dataset(\n                num_parallel_calls=3,\n                sample_batch_size=hyper_parameters['batch_size'],\n                num_steps=2).prefetch(3)\n\n            iterator = iter(dataset)\n\n            # (Optional) Optimize by wrapping some code in a graph using TF function.\n            self.agent.train = common.function(self.agent.train)\n\n            # Reset the train step.\n            self.agent.train_step_counter.assign(0)\n\n            # Evaluate the agent's policy once before training.\n            avg_return = self.compute_avg_return(self.eval_env, self.agent.policy,\n                                                 hyper_parameters['num_eval_episodes'])\n            returns = [avg_return]\n\n            # Reset the environment.\n            time_step = train_py_env.reset()\n\n            # Create a driver to collect experience.\n            collect_driver = py_driver.PyDriver(\n                self.env,\n                py_tf_eager_policy.PyTFEagerPolicy(\n                    self.agent.collect_policy, use_tf_function=True),\n                [rb_observer],\n                max_steps=hyper_parameters['collect_steps_per_iteration'])\n\n            for _ in range(hyper_parameters['num_iterations']):\n                # Collect a few steps and save to the replay buffer.\n                time_step, _ = collect_driver.run(time_step)\n\n                # Sample a batch of data from the buffer and update the agent's network.\n                experience, unused_info = next(iterator)\n                train_loss = self.agent.train(experience).loss\n\n                step = self.agent.train_step_counter.numpy()\n\n                if step % hyper_parameters['log_interval'] == 0:\n                    print('step = {0}: loss = {1}'.format(step, train_loss))\n\n                if step % hyper_parameters['eval_interval'] == 0:\n                    # Agent Policy Output\n                    avg_return = self.compute_avg_return(self.eval_env, self.agent.policy,\n                                                         hyper_parameters['num_eval_episodes'])\n                    returns.append(avg_return)\n                    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n                    if ('max_train_reward' in hyper_parameters) and\\\n                            (avg_return > hyper_parameters['max_train_reward']):\n                        break\n\n            print('\\n-----\\nTraining complete\\n-----')\n\n    def choose_actions(self, tracks, sensors, timestamp, nchoose=1, **kwargs):\n        \"\"\"Returns a chosen [list of] action(s) from the action set for each sensor.\n        Chosen action(s) is selected by exploiting the reinforcement learning agent's\n        policy that was found during training.\n\n        Parameters\n        ----------\n        tracks: set of :class:`~Track`\n            Set of tracks at given time. Used in reward function.\n        sensors: :class:`~Sensor`\n            Sensor(s) used for observation\n        timestamp: :class:`tf_agents.trajectories.TimeSpec`\n            Timestep of environment at current time\n        nchoose : int\n            Number of actions from the set to choose (default is 1)\n\n        Returns\n        -------\n        : dict\n            The pairs of :class:`~.Sensor`: [:class:`~.Action`] selected\n        \"\"\"\n\n        configs = [dict() for _ in range(nchoose)]\n        for sensor_action_assignment in configs:\n            for sensor in sensors:\n                chosen_actions = []\n                action_step = self.agent.policy.action(timestamp)\n                action = action_step.action\n                stonesoup_action = self.env.generate_action(action, tracks, sensor)\n                chosen_actions.append(stonesoup_action)\n                sensor_action_assignment[sensor] = chosen_actions\n\n            return configs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Sensor Managers\nWe initiate our reinforcement learning sensor manager with the environment we have designed\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.sensormanager import BruteForceSensorManager\nreinforcementsensormanager = ReinforcementSensorManager({sensorA}, env=StoneSoupEnv())\nbruteforcesensormanager = BruteForceSensorManager({sensorB}, reward_function=reward_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train RL agent\nTo generate a policy, we need to train the reinforcement learning agent using the environment we created above.\nSome hyperparameters are created that the agent uses to train with.\n\nTo train the agent, the hyperparameters are passed to the train method in the :class:`~.ReinforcementSensorManager`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\nimport reverb\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.drivers import py_driver\nfrom tf_agents.networks import sequential\nfrom tf_agents.policies import py_tf_eager_policy, random_tf_policy\nfrom tf_agents.replay_buffers import reverb_replay_buffer, reverb_utils\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.utils import common\n\nnum_iterations = 10000\n\ninitial_collect_steps = 100\ncollect_steps_per_iteration = 1\nreplay_buffer_max_length = 100000\n\nbatch_size = 64\nlearning_rate = 1e-4\nlog_interval = 500\n\nnum_eval_episodes = 10\neval_interval = 1000\nfc_layer_params = (100, 50)\n\n# ---- Optional ----\nmax_train_reward = 250\n\nhyper_parameters = {'num_iterations': num_iterations,\n                    'initial_collect_steps': initial_collect_steps,\n                    'collect_steps_per_iteration': collect_steps_per_iteration,\n                    'replay_buffer_max_length': replay_buffer_max_length,\n                    'batch_size': batch_size,\n                    'learning_rate': learning_rate,\n                    'log_interval': log_interval,\n                    'num_eval_episodes': num_eval_episodes,\n                    'eval_interval': eval_interval,\n                    'fc_layer_params': fc_layer_params,\n                    'max_train_reward': max_train_reward}\n\nreinforcementsensormanager.train(hyper_parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the sensor managers\nTo be able to exploit the policy generated by the reinforcement sensor manager, it  must be passed appropriate\n'timesteps'.\nThese are distinct from the timesteps in Stonesoup, and is of the form time_step_spec from tf-agents.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n\nsensor_history_A = dict()\n\ntimesteps = []\nfor state in truths[0]:\n    timesteps.append(state.timestamp)\n\ntf_timestep = reinforcementsensormanager.test_env.reset()\nreinforcementsensormanager.env.reset()\nfor timestep in timesteps[1:]:\n\n    # Generate chosen configuration\n    # i.e. {a}k\n    # Need to make our own \"timestamp\" that matches tensorflow time_step_spec\n    observation = []\n    uncertainty = []\n    for target in tracksA:\n        x_target = target.state.state_vector[0]-sensorA.position[0]\n        y_target = target.state.state_vector[2]-sensorA.position[1]\n        bearing_target = mod_bearing(np.arctan2(y_target, x_target))\n        uncertainty.append(np.trace(target.covar))\n\n        # observation.append(np.degrees(bearing_target))\n        observation.append(np.trace(target.covar))\n\n    observation = np.array(uncertainty, dtype=np.float32)\n    # observation = np.array(observation, dtype=np.float32)\n\n    chosen_actions = reinforcementsensormanager.choose_actions(tracksA, [sensorA], tf_timestep)\n\n    # Create empty dictionary for measurements\n    measurementsA = []\n\n    for chosen_action in chosen_actions:\n        # chosen_action is a pair of {sensor, action}\n        for sensor, actions in chosen_action.items():\n            sensor.add_actions(list(chain.from_iterable(actions)))\n\n    sensorA.act(timestep)\n\n    # Store sensor history for plotting\n    sensor_history_A[timestep] = copy.copy(sensorA)\n\n    # Observe this ground truth\n    # i.e. {z}k\n    measurements = sensorA.measure(OrderedSet(truth[timestep] for truth in truths), noise=True)\n    measurementsA.extend(measurements)\n\n    hypotheses = data_associator.associate(tracksA,\n                                           measurementsA,\n                                           timestep)\n    for track in tracksA:\n        hypothesis = hypotheses[track]\n        if hypothesis.measurement:\n            post = updater.update(hypothesis)\n            track.append(post)\n        else:  # When data associator says no detections are good enough, we'll keep the prediction\n            track.append(hypothesis.prediction)\n\n    # Propagate environment\n    action_step = reinforcementsensormanager.agent.policy.action(tf_timestep)\n    tf_timestep = reinforcementsensormanager.test_env.step(action_step.action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot ground truths, tracks and uncertainty ellipses for each target.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\nfrom stonesoup.functions import pol2cart\n\nplotterA = AnimatedPlotterly(timesteps, tail_length=1, sim_duration=10)\nplotterA.plot_sensors(sensorA)\nplotterA.plot_ground_truths(truths, [0, 2])\nplotterA.plot_tracks(tracksA, [0, 2], uncertainty=True, plot_history=False)\n\n\ndef plot_sensor_fov(fig, sensor_history):\n    # Plot sensor field of view\n    trace_base = len(fig.data)\n    fig.add_trace(go.Scatter(mode='lines',\n                             line=go.scatter.Line(color='black',\n                                                  dash='dash')))\n\n    for frame in fig.frames:\n        traces_ = list(frame.traces)\n        data_ = list(frame.data)\n        x = [0, 0]\n        y = [0, 0]\n        timestring = frame.name\n        timestamp = datetime.strptime(timestring, \"%Y-%m-%d %H:%M:%S\")\n\n        if timestamp in sensor_history:\n            sensor = sensor_history[timestamp]\n            for i, fov_side in enumerate((-1, 1)):\n                range_ = min(getattr(sensor, 'max_range', np.inf), 100)\n                x[i], y[i] = pol2cart(range_,\n                                      sensor.dwell_centre[0, 0]\n                                      + sensor.fov_angle / 2 * fov_side) \\\n                             + sensor.position[[0, 1], 0]\n        else:\n            continue\n\n        data_.append(go.Scatter(x=[x[0], sensor.position[0], x[1]],\n                                y=[y[0], sensor.position[1], y[1]],\n                                mode=\"lines\",\n                                line=go.scatter.Line(color='black',\n                                                     dash='dash'),\n                                showlegend=False))\n        traces_.append(trace_base)\n        frame.traces = traces_\n        frame.data = data_\n\n\nplot_sensor_fov(plotterA.fig, sensor_history_A)\nplotterA.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run brute force sensor manager\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sensor_history_B = dict()\nfor timestep in timesteps[1:]:\n\n    # Generate chosen configuration\n    # i.e. {a}k\n    chosen_actions = bruteforcesensormanager.choose_actions(tracksB, timestep)\n\n    # Create empty dictionary for measurements\n    measurementsB = set()\n\n    for chosen_action in chosen_actions:\n        for sensor, actions in chosen_action.items():\n            sensor.add_actions(actions)\n\n    sensorB.act(timestep)\n\n    # Store sensor history for plotting\n    sensor_history_B[timestep] = copy.copy(sensorB)\n\n    # Observe this ground truth\n    # i.e. {z}k\n    measurementsB |= sensorB.measure(OrderedSet(truth[timestep] for truth in truths), noise=True)\n\n    hypotheses = data_associator.associate(tracksB,\n                                           measurementsB,\n                                           timestep)\n    for track in tracksB:\n        hypothesis = hypotheses[track]\n        if hypothesis.measurement:\n            post = updater.update(hypothesis)\n            track.append(post)\n        else:  # When data associator says no detections are good enough, we'll keep the prediction\n            track.append(hypothesis.prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot ground truths, tracks and uncertainty ellipses for each target.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotterB = AnimatedPlotterly(timesteps, tail_length=1, sim_duration=10)\nplotterB.plot_sensors(sensorB)\nplotterB.plot_ground_truths(truths, [0, 2])\nplotterB.plot_tracks(tracksB, [0, 2], uncertainty=True, plot_history=False)\nplot_sensor_fov(plotterB.fig, sensor_history_B)\nplotterB.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With a properly trained policy, the :class:`~.ReinforcementSensorManager` performs almost as well as the\n:class:`~.BruteForceSensorManager`. Also, once the policy has been learnt, the time taken to run the\ntracking loop is far smaller for the :class:`~.ReinforcementSensorManager` than for the\n:class:`~.BruteForceSensorManager`, which must re-calculate the best actions each time it is run.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics\nMetrics can be used to compare how well different sensor management techniques are working.\nFull explanations of the OSPA\nand SIAP metrics can be found in the Metrics Example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.metricgenerator.ospametric import OSPAMetric\nospa_generatorA = OSPAMetric(c=40, p=1,\n                             generator_name='ReinforcementSensorManager',\n                             tracks_key='tracksA',\n                             truths_key='truths')\n\nospa_generatorB = OSPAMetric(c=40, p=1,\n                             generator_name='BruteForceSensorManager',\n                             tracks_key='tracksB',\n                             truths_key='truths')\n\nfrom stonesoup.metricgenerator.tracktotruthmetrics import SIAPMetrics\nfrom stonesoup.measures import Euclidean\nsiap_generatorA = SIAPMetrics(position_measure=Euclidean((0, 2)),\n                              velocity_measure=Euclidean((1, 3)),\n                              generator_name='Reinforcement',\n                              tracks_key='tracksA',\n                              truths_key='truths')\n\nsiap_generatorB = SIAPMetrics(position_measure=Euclidean((0, 2)),\n                              velocity_measure=Euclidean((1, 3)),\n                              generator_name='BruteForceSensorManager',\n                              tracks_key='tracksB',\n                              truths_key='truths')\n\nfrom stonesoup.dataassociator.tracktotrack import TrackToTruth\nassociator = TrackToTruth(association_threshold=30)\n\nfrom stonesoup.metricgenerator.uncertaintymetric import SumofCovarianceNormsMetric\nuncertainty_generatorA = SumofCovarianceNormsMetric(generator_name='ReinforcementSensorManager',\n                                                    tracks_key='tracksA')\n\nuncertainty_generatorB = SumofCovarianceNormsMetric(generator_name='BruteForceSensorManager',\n                                                    tracks_key='tracksB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate a metrics manager.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.metricgenerator.manager import MultiManager\n\nmetric_manager = MultiManager([ospa_generatorA,\n                               ospa_generatorB,\n                               siap_generatorA,\n                               siap_generatorB,\n                               uncertainty_generatorA,\n                               uncertainty_generatorB],\n                               associator=associator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each time step, data is added to the metric manager on truths and tracks. The metrics themselves can then be\ngenerated from the metric manager.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metric_manager.add_data({'truths': truths, 'tracksA': tracksA, 'tracksB': tracksB})\n\nmetrics = metric_manager.generate_metrics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OSPA metric\nFirst we look at the OSPA metric. This is plotted over time for each sensor manager method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.plotter import MetricPlotter\n\nfig = MetricPlotter()\nfig.plot_metrics(metrics, metric_names=['OSPA distances'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :class:`~.BruteForceSensorManager` generally results in a smaller OSPA distance\nthan the observations of the :class:`~.ReinforcementSensorManager`, reflecting the better tracking performance\nseen in the tracking plots.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SIAP metrics\nNext we look at SIAP metrics. We are only interested in the positional accuracy (PA) and velocity accuracy (VA).\nThese metrics can be plotted to show how they change over time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig2 = MetricPlotter()\nfig2.plot_metrics(metrics, metric_names=['SIAP Position Accuracy at times',\n                                         'SIAP Velocity Accuracy at times'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to the OSPA distances the :class:`~.BruteForceSensorManager`\ngenerally results in both a better positional accuracy and velocity accuracy than the observations\nof the :class:`~.ReinforcementSensorManager`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Uncertainty metric\nFinally we look at the uncertainty metric which computes the sum of covariance matrix norms of each state at each\ntime step. This is plotted over time for each sensor manager method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig3 = MetricPlotter()\nfig3.plot_metrics(metrics, metric_names=['Sum of Covariance Norms Metric'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This metric shows that the uncertainty in the tracks generated by the :class:`~.ReinforcementSensorManager` is a\nlittle higher than for those generated by the :class:`~.BruteForceSensorManager`. This is also reflected by the\nuncertainty ellipses in the initial plots of tracks and truths.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n.. [#] *https://www.tensorflow.org/install/pip#windows-wsl2*\n.. [#] *https://github.com/tensorflow/agents/blob/master/docs/tutorials/2_environments_tutorial.ipynb*\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}