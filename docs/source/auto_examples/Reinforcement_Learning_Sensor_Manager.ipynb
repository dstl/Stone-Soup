{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Reinforcement Learning Sensor Manager\nThis example looks at how to interface a reinforcement learning framework with a Stone Soup sensor manager.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making a Reinforcement Learning Sensor Manager\nThis example introduces using a Deep Q Network (DQN) reinforcement learning (RL) sensor management algorithm\nin Stone Soup. This is compared to the performance of a brute force algorithm using the same metrics shown in the\nsensor management tutorials. This example is similar to the sensor management tutorials, simulating 3 targets and a\n:class:`~.RadarRotatingBearingRange` sensor which can be actioned to point in different directions.\n\nTensorflow-agents is used as the reinforcement learning framework. This is a separate python package that can be found\nat https://github.com/tensorflow/agents.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>This currently only works on Linux based OSes, or via Windows Subsystem for\n   Linux (WSL). MacOS users may be able to make use of a Linux VM to run this example. See Tensorflow instructions for\n   creating Python virtual environments (with GPU support if applicable) [#]_.</p></div>\n\n\nTo run this example, in a clean environment, do  ``pip install stonesoup``, followed by ``pip install\ntf-agents[reverb]``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Some general imports and set up\nimport numpy as np\nimport random\nfrom datetime import datetime, timedelta\n\ntry:\n    import reverb\nexcept ImportError:\n    raise ImportError('To run this example, reverb must be installed. Please read the warning'\n                      'and instructions at the top of this notebook.')\n\nstart_time = datetime.now().replace(microsecond=0)\n\nfrom stonesoup.models.transition.linear import CombinedLinearGaussianTransitionModel, ConstantVelocity\nfrom stonesoup.types.groundtruth import GroundTruthPath, GroundTruthState"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate ground truths\nFollowing the methods from previous Stone Soup sensor management tutorials, generate a series of combined linear\nGaussian transition models and generate ground truths. Each ground truth is offset in the y-direction by 10.\n\nThe number of targets in this simulation is defined by `ntruths` - here there are 3 targets travelling in different\ndirections. The time the simulation is observed for is defined by `time_max`.\n\nWe can fix our random number generator in order to probe a particular example repeatedly. To produce random examples,\ncomment out the next two lines.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(1990)\nrandom.seed(1990)\n\n# Generate transition model\n# i.e. fk(xk|xk-1)\ntransition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(0.005),\n                                                          ConstantVelocity(0.005)])\n\nyps = range(0, 100, 10)  # y value for prior state\ntruths = []\nntruths = 3  # number of ground truths in simulation\ntime_max = 50  # timestamps the simulation is observed over\ntimesteps = [start_time + timedelta(seconds=k) for k in range(time_max)]\n\nxdirection = 1\nydirection = 1\n\n# Generate ground truths\nfor j in range(0, ntruths):\n    truth = GroundTruthPath([GroundTruthState([0, xdirection, yps[j], ydirection],\n                                              timestamp=start_time)],\n                            id=f\"id{j}\")\n\n    for k in range(1, time_max):\n        truth.append(\n            GroundTruthState(transition_model.function(truth[k - 1], noise=True, time_interval=timedelta(seconds=1)),\n                             timestamp=start_time + timedelta(seconds=k)))\n    truths.append(truth)\n\n    # alternate directions when initiating tracks\n    xdirection *= -1\n    if j % 2 == 0:\n        ydirection *= -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the ground truths. This is done using the :class:`~.Plotterly` class from Stone Soup.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.plotter import AnimatedPlotterly\n\nplotter = AnimatedPlotterly(timesteps, tail_length=1)\nplotter.plot_ground_truths(truths, [0, 2])\nplotter.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create sensors\nCreate a sensor for each sensor management algorithm. This tutorial uses the\n:class:`~.RadarRotatingBearingRange` sensor. This sensor is an :class:`~.Actionable` so\nis capable of returning the actions it can take at a given time step and can also be given an action to take before\nmeasuring.\nSee the :doc:`Creating an Actionable Sensor Example <Creating_Actionable_Sensor>` for a more\ndetailed explanation of actionable sensors.\n\nThe :class:`~.RadarRotatingBearingRange` has a dwell centre which is an :class:`~.ActionableProperty`\nso in this case the action is changing the dwell centre to point in a specific direction.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.types.state import StateVector\nfrom stonesoup.sensor.radar.radar import RadarRotatingBearingRange\n\nsensorA = RadarRotatingBearingRange(\n    position_mapping=(0, 2),\n    noise_covar=np.array([[np.radians(0.5) ** 2, 0],\n                          [0, 1 ** 2]]),\n    ndim_state=4,\n    position=np.array([[10], [0]]),\n    rpm=60,\n    fov_angle=np.radians(45),\n    dwell_centre=StateVector([0.0]),\n    max_range=np.inf\n)\nsensorA.timestamp = start_time\n\nsensorB = RadarRotatingBearingRange(\n    position_mapping=(0, 2),\n    noise_covar=np.array([[np.radians(0.5) ** 2, 0],\n                          [0, 1 ** 2]]),\n    ndim_state=4,\n    position=np.array([[10], [0]]),\n    rpm=60,\n    fov_angle=np.radians(45),\n    dwell_centre=StateVector([0.0]),\n    max_range=np.inf\n)\nsensorB.timestamp = start_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the Kalman predictor and updater\nConstruct a predictor and updater using the :class:`~.KalmanPredictor` and :class:`~.ExtendedKalmanUpdater`\ncomponents from Stone Soup. The :class:`~.ExtendedKalmanUpdater` is used because it can be used for both linear\nand nonlinear measurement models. A hypothesiser and data associator are required for use in both trackers.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.predictor.kalman import KalmanPredictor\n\npredictor = KalmanPredictor(transition_model)\n\nfrom stonesoup.updater.kalman import ExtendedKalmanUpdater\n\nupdater = ExtendedKalmanUpdater(measurement_model=None)\n# measurement model is added to detections by the sensor\n\nfrom stonesoup.hypothesiser.distance import DistanceHypothesiser\nfrom stonesoup.measures import Mahalanobis\n\nhypothesiser = DistanceHypothesiser(predictor, updater, measure=Mahalanobis(), missed_distance=5)\n\nfrom stonesoup.dataassociator.neighbour import GNNWith2DAssignment\n\ndata_associator = GNNWith2DAssignment(hypothesiser)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Priors\nFirst create `ntruths` priors which estimate the targets\u2019 initial states, one for each target. In this example\neach prior is offset by 0.1 in the y direction meaning the position of the track is initially not very accurate. The\nvelocity is also systematically offset by +0.2 in both the x and y directions.\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.types.state import GaussianState\n\npriors = []\nxdirection = 1.2\nydirection = 1.2\nfor j in range(0, ntruths):\n    priors.append(GaussianState([[0], [xdirection], [yps[j] + 0.1], [ydirection]],\n                                np.diag([0.5, 0.5, 0.5, 0.5] + np.random.normal(0, 5e-4, 4)),\n                                timestamp=start_time))\n    xdirection *= -1\n    if j % 2 == 0:\n        ydirection *= -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialise the tracks by creating an empty list and appending the priors generated. This needs to be done separately\nfor both sensor manager methods as they will generate different sets of tracks.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.types.track import Track\n\n# Initialise tracks from the RandomSensorManager\ntracksA = []\nfor j, prior in enumerate(priors):\n    tracksA.append(Track([prior]))\n\ntracksB = []\nfor j, prior in enumerate(priors):\n    tracksB.append(Track([prior]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reward function\nA reward function is used to quantify the benefit of sensors taking a particular action or set of actions.\nThis can be crafted specifically for an example in order to achieve a particular objective. The function used in\nthis example is quite generic but could be substituted for any callable function which returns a numeric\nvalue that the sensor manager can maximise.\n\nThe :class:`~.UncertaintyRewardFunction` calculates the uncertainty reduction by computing the difference between the\ncovariance matrix norms of the prediction, and the posterior assuming a predicted measurement corresponding to that\nprediction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.sensormanager.reward import UncertaintyRewardFunction\n\nreward_function = UncertaintyRewardFunction(predictor=predictor, updater=updater)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reinforcement Learning\nReinforcement learning involves intelligent agents making decisions to maximise a cumulative reward. The agent\nmust train in an environment in order to create a policy, which later determines the actions it will take. During\ntraining, the agent makes decisions and receives rewards, which it uses to optimise the policy.\n\n.. figure:: ../_static/rl_training.png\n  :width: 800\n  :alt: Illustration of sequential actions and measurements\n\n  Illustration of an RL algorithm taking actions during training. The state and reward it receives are used to\n  determine the best actions.\n\nOnce training has completed, the policy can be exploited to gain rewards.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Design Environment\nAn environment is needed for the RL agent to learn in. There are resources online for how to design these [#]_.\n\nIn this example, the action space is equal to the number of targets in the simulation, so at each time step, the\nsensor can select one target to look at. For the environment, we make a copy of the sensor that we will pass to the\nsensor manager later on. This is so the agent can train in the environment without altering the sensor itself.\nThe :class:`~.UncertaintyRewardFunction` is used to calculate the reward obtained for each step in the environment.\nThe trace of the covariances for each object is used as the observation for the agent to learn from - it should learn\nto select targets with a larger covariance (higher uncertainty).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from abc import ABC\nimport numpy as np\nimport copy\nfrom ordered_set import OrderedSet\n\nfrom stonesoup.sensor.action.dwell_action import DwellActionsGenerator\nfrom stonesoup.functions import mod_bearing\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.environments import utils\n\n\nclass StoneSoupEnv(py_environment.PyEnvironment, ABC):\n    \"\"\"Example reinforcement learning environment. Environments must contain __init__, _reset,\n    _step, and generate_action methods\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # Action size is number of targets\n        self._action_spec = array_spec.BoundedArraySpec(\n            shape=(), dtype=np.int32, minimum=0, maximum=ntruths - 1, name='action')\n        # Observation size is also number of targets\n        self.obs_size = ntruths\n        self._observation_spec = array_spec.BoundedArraySpec(\n            shape=(self.obs_size,), dtype=np.float32, name='observation')\n        self._episode_ended = False\n        self.max_episode_length = time_max\n        self.current_step = 0\n        self.start_time = start_time\n        # Use deepcopy to prevent the original sensor/tracks being changed each time an episode is run\n        self.sensor = copy.deepcopy(sensorA)\n        self.sensor.timestamp = start_time\n        self.tracks = copy.deepcopy(tracksA)\n\n    def action_spec(self):\n        \"\"\"Return action_spec.\"\"\"\n        return self._action_spec\n\n    def observation_spec(self):\n        \"\"\"Return observation_spec.\"\"\"\n        return self._observation_spec\n\n    def _reset(self):\n        \"\"\"Restarts the environment from the first step, resets the initial state\n        and observation values, and returns an initial observation\n        \"\"\"\n        self._episode_ended = False\n        self.current_step = 0\n        self.sensor = copy.deepcopy(sensorA)\n        self.sensor.timestamp = start_time\n        self.tracks = copy.deepcopy(tracksA)\n        return ts.restart(np.zeros((self.obs_size,), dtype=np.float32))\n\n    def _step(self, action):\n        \"\"\"Apply action and take one step through environment, and return new time_step.\n        \"\"\"\n\n        reward = 0\n        if self._episode_ended:\n            # The last action ended the episode. Ignore the current action and start\n            # a new episode.\n            return self.reset()\n\n        uncertainty = []\n        for i, target in enumerate(self.tracks):\n            # Calculate the bearing of the chosen target from the sensor\n            if i == action:\n                x_target = target.state.state_vector[0] - self.sensor.position[0]\n                y_target = target.state.state_vector[2] - self.sensor.position[1]\n                bearing_target = mod_bearing(np.arctan2(y_target, x_target))\n\n            uncertainty.append(np.trace(target.covar))\n\n        current_timestep = self.start_time + timedelta(seconds=self.current_step)\n        next_timestep = self.start_time + timedelta(seconds=self.current_step + 1)\n\n        # Create action generator which contains possible actions\n        action_generator = DwellActionsGenerator(self.sensor,\n                                                 attribute='dwell_centre',\n                                                 start_time=current_timestep,\n                                                 end_time=next_timestep)\n\n        # Action the environment's sensor to point towards the chosen target\n        current_action = [action_generator.action_from_value(bearing_target)]\n        config = ({self.sensor: current_action})\n        reward += reward_function(config, self.tracks, next_timestep)\n\n        self.sensor.add_actions(current_action)\n        self.sensor.act(next_timestep)\n\n        # Calculate a measurement from the sensor\n        measurement = set()\n        measurement |= self.sensor.measure(OrderedSet(truth[current_timestep] for truth in truths), noise=True)\n        hypotheses = data_associator.associate(self.tracks,\n                                               measurement,\n                                               current_timestep)\n\n        for track in self.tracks:\n            hypothesis = hypotheses[track]\n            if hypothesis.measurement:\n                post = updater.update(hypothesis)\n                track.append(post)\n            else:  # When data associator says no detections are good enough, we'll keep the prediction\n                track.append(hypothesis.prediction)\n\n        # Set the observation as the prior uncertainty of each target\n        observation = np.array(uncertainty, dtype=np.float32)\n\n        self.current_step += 1\n\n        if self.current_step >= self.max_episode_length - 1:\n            self._episode_ended = True\n            return ts.termination(observation, reward)\n        else:\n            return ts.transition(observation, reward=reward, discount=1.0)\n\n    @staticmethod\n    def generate_action(action, tracks, sensor):\n        \"\"\"This method is used to convert a tf-agents action into a Stone Soup action\"\"\"\n        for i, target in enumerate(tracks):\n            if i == action:\n                x_target = target.state.state_vector[0] - sensor.position[0]\n                y_target = target.state.state_vector[2] - sensor.position[1]\n                action_bearing = mod_bearing(np.arctan2(y_target, x_target))\n\n        action_generators = DwellActionsGenerator(sensor,\n                                                  attribute='dwell_centre',\n                                                  start_time=sensor.timestamp,\n                                                  end_time=sensor.timestamp + timedelta(seconds=1))\n\n        current_action = [action_generators.action_from_value(action_bearing)]\n        return current_action\n\n\n# Validate the environment to ensure that the environment returns the expected specs\ntrain_env = StoneSoupEnv()\nutils.validate_py_environment(train_env, episodes=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RL Sensor Manager\nTo be able to use the RL environment we have designed, we need to make a ReinforcementSensorManager class, which\ninherits from :class:`~.SensorManager`.\n\nWe introduce some additional methods that are used by tensorflow-agents: :func:`compute_avg_return`,\n:func:`dense_layer`, and :func:`train`.\n:func:`compute_avg_return` is used to find the average reward by using a given policy. This is used to evaluate the\ntraining.\n:func:`dense_layer` is used when generating the Q-Network, a neural network model that learns to predict Q-Values.\n:func:`train` is used to generate the policy by running a large number of episodes through the Q-Network to work out\nwhich actions are best. An episode in RL refers to a single run or instance of the learning process, where the agent\ninteracts with the environment.\n\nWe also need to re-define the :func:`choose_actions` method from :class:`~.SensorManager` to be able to interface\nStone Soup actions with tensorflow-agent actions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.sensormanager.base import SensorManager\nfrom stonesoup.base import Property\nfrom tf_agents.environments import tf_py_environment\n\n\nclass ReinforcementSensorManager(SensorManager):\n    \"\"\"A sensor manager that employs reinforcement learning algorithms from tensorflow-agents.\n    The sensor manager trains on an environment to find an optimal policy, which is then exploited\n    to choose actions.\n    \"\"\"\n    env: py_environment.PyEnvironment = Property(doc=\"The environment which the agent learns the policy with.\")\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.tf_env = tf_py_environment.TFPyEnvironment(self.env)\n        self.test_env = tf_py_environment.TFPyEnvironment(self.env)\n        self.agent = None\n\n    @staticmethod\n    def compute_avg_return(environment, policy, num_episodes=10):\n        \"\"\"Used to calculate the average reward over a set of episodes.\n\n        Parameters\n        ----------\n        environment:\n            tf-agents environment for evaluating policy on\n\n        policy:\n            tf-agents policy for choosing actions in environment\n\n        num_episodes: int\n            Number of episodes to sample over\n\n        Returns\n        -------\n        : int\n            average reward calculated over num_episodes\n\n        \"\"\"\n        time_step = None\n        episode_return = None\n        total_return = 0.0\n        for _ in range(num_episodes):\n            time_step = environment.reset()\n            episode_return = 0.0\n\n            while not time_step.is_last():\n                action_step = policy.action(time_step)\n                time_step = environment.step(action_step.action)\n                episode_return += time_step.reward\n            total_return += episode_return\n\n        avg_return = total_return / num_episodes\n        return avg_return.numpy()[0]\n\n    @staticmethod\n    def dense_layer(num_units):\n        \"\"\"Method for generating fully connected layers for use in the neural network.\n\n        Parameters\n        ----------\n        num_units: int\n            Number of nodes in dense layer\n\n        Returns\n        -------\n        : tensorflow dense layer\n\n        \"\"\"\n        # Define a helper function to create Dense layers configured with the right\n        # activation and kernel initializer.\n        return tf.keras.layers.Dense(\n            num_units,\n            activation=tf.keras.activations.relu,\n            kernel_initializer=tf.keras.initializers.VarianceScaling(\n                scale=2.0, mode='fan_in', distribution='truncated_normal'))\n\n    def train(self, hyper_parameters):\n        \"\"\"Trains a DQN agent on the specified environment to learn a policy that is later\n        used to select actions.\n\n        Parameters\n        ----------\n        hyper_parameters: dict\n            Dictionary containing hyperparameters used in training. See tutorial for\n            necessary hyperparameters.\n\n        \"\"\"\n        if self.env is not None:\n            self.env.reset()\n\n            train_py_env = self.env\n            eval_py_env = self.env\n            self.train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n            self.eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\n            fc_layer_params = hyper_parameters['fc_layer_params']\n            action_tensor_spec = tensor_spec.from_spec(self.env.action_spec())\n            num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n\n            # QNetwork consists of a sequence of Dense layers followed by a dense layer\n            # with `num_actions` units to generate one q_value per available action as\n            # its output.\n\n            dense_layers = [self.dense_layer(num_units) for num_units in fc_layer_params]\n            q_values_layer = tf.keras.layers.Dense(\n                num_actions,\n                activation=None,\n                kernel_initializer=tf.keras.initializers.RandomUniform(\n                    minval=-0.03, maxval=0.03),\n                bias_initializer=tf.keras.initializers.Constant(-0.2))\n            q_net = sequential.Sequential(dense_layers + [q_values_layer])\n\n            optimizer = tf.keras.optimizers.Adam(hyper_parameters['learning_rate'])\n\n            train_step_counter = tf.Variable(0)\n\n            self.agent = dqn_agent.DdqnAgent(\n                self.train_env.time_step_spec(),\n                self.train_env.action_spec(),\n                q_network=q_net,\n                optimizer=optimizer,\n                td_errors_loss_fn=common.element_wise_squared_loss,\n                train_step_counter=train_step_counter)\n\n            self.agent.initialize()\n\n            random_policy = random_tf_policy.RandomTFPolicy(self.train_env.time_step_spec(),\n                                                            self.train_env.action_spec())\n\n            # See also the metrics module for standard implementations of different metrics.\n            # https://github.com/tensorflow/agents/tree/master/tf_agents/metrics\n\n            self.compute_avg_return(self.eval_env, random_policy,\n                                    hyper_parameters['num_eval_episodes'])\n\n            table_name = 'uniform_table'\n            replay_buffer_signature = tensor_spec.from_spec(\n                self.agent.collect_data_spec)\n            replay_buffer_signature = tensor_spec.add_outer_dim(\n                replay_buffer_signature)\n\n            table = reverb.Table(\n                table_name,\n                max_size=hyper_parameters['replay_buffer_max_length'],\n                sampler=reverb.selectors.Uniform(),\n                remover=reverb.selectors.Fifo(),\n                rate_limiter=reverb.rate_limiters.MinSize(1),\n                signature=replay_buffer_signature)\n\n            reverb_server = reverb.Server([table])\n\n            replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n                self.agent.collect_data_spec,\n                table_name=table_name,\n                sequence_length=2,\n                local_server=reverb_server)\n\n            rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n                replay_buffer.py_client,\n                table_name,\n                sequence_length=2)\n\n            py_driver.PyDriver(\n                self.env,\n                py_tf_eager_policy.PyTFEagerPolicy(\n                    random_policy, use_tf_function=True),\n                [rb_observer],\n                max_steps=hyper_parameters['initial_collect_steps']).run(train_py_env.reset())\n\n            # Dataset generates trajectories with shape [Bx2x...]\n            dataset = replay_buffer.as_dataset(\n                num_parallel_calls=3,\n                sample_batch_size=hyper_parameters['batch_size'],\n                num_steps=2).prefetch(3)\n\n            iterator = iter(dataset)\n\n            # (Optional) Optimize by wrapping some code in a graph using TF function.\n            self.agent.train = common.function(self.agent.train)\n\n            # Reset the train step.\n            self.agent.train_step_counter.assign(0)\n\n            # Evaluate the agent's policy once before training.\n            avg_return = self.compute_avg_return(self.eval_env, self.agent.policy,\n                                                 hyper_parameters['num_eval_episodes'])\n            returns = [avg_return]\n\n            # Reset the environment.\n            time_step = train_py_env.reset()\n\n            # Create a driver to collect experience.\n            collect_driver = py_driver.PyDriver(\n                self.env,\n                py_tf_eager_policy.PyTFEagerPolicy(\n                    self.agent.collect_policy, use_tf_function=True),\n                [rb_observer],\n                max_steps=hyper_parameters['collect_steps_per_iteration'])\n\n            for _ in range(hyper_parameters['num_iterations']):\n                # Collect a few steps and save to the replay buffer.\n                time_step, _ = collect_driver.run(time_step)\n\n                # Sample a batch of data from the buffer and update the agent's network.\n                experience, unused_info = next(iterator)\n                train_loss = self.agent.train(experience).loss\n\n                step = self.agent.train_step_counter.numpy()\n\n                if step % hyper_parameters['log_interval'] == 0:\n                    print('step = {0}: loss = {1}'.format(step, train_loss))\n\n                if step % hyper_parameters['eval_interval'] == 0:\n                    # Agent Policy Output\n                    avg_return = self.compute_avg_return(self.eval_env, self.agent.policy,\n                                                         hyper_parameters['num_eval_episodes'])\n                    returns.append(avg_return)\n                    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n                    if ('max_train_reward' in hyper_parameters) and \\\n                            (avg_return > hyper_parameters['max_train_reward']):\n                        break\n\n            print('\\n-----\\nTraining complete\\n-----')\n\n    def choose_actions(self, tracks, sensors, timestamp, nchoose=1, **kwargs):\n        \"\"\"Returns a chosen [list of] action(s) from the action set for each sensor.\n        Chosen action(s) is selected by exploiting the reinforcement learning agent's\n        policy that was found during training.\n\n        Parameters\n        ----------\n        tracks: set of :class:`~Track`\n            Set of tracks at given time. Used in reward function.\n        sensors: :class:`~Sensor`\n            Sensor(s) used for observation\n        timestamp: :class:`tf_agents.trajectories.TimeSpec`\n            Timestep of environment at current time\n        nchoose : int\n            Number of actions from the set to choose (default is 1)\n\n        Returns\n        -------\n        : dict\n            The pairs of :class:`~.Sensor`: [:class:`~.Action`] selected\n        \"\"\"\n\n        configs = [dict() for _ in range(nchoose)]\n        for sensor_action_assignment in configs:\n            for sensor in sensors:\n                chosen_actions = []\n                action_step = self.agent.policy.action(timestamp)\n                action = action_step.action\n                stonesoup_action = self.env.generate_action(action, tracks, sensor)\n                chosen_actions.append(stonesoup_action)\n                sensor_action_assignment[sensor] = chosen_actions\n\n            return configs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Sensor Managers\nWe initiate our reinforcement learning sensor manager with the environment we have designed\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.sensormanager import BruteForceSensorManager\n\nreinforcementsensormanager = ReinforcementSensorManager({sensorA}, env=StoneSoupEnv())\nbruteforcesensormanager = BruteForceSensorManager({sensorB}, reward_function=reward_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train RL agent\nTo generate a policy, we need to train the reinforcement learning agent using the environment we created above.\nSome hyperparameters are created that the agent uses to train with.\n\nTo train the agent, the hyperparameters are passed to the train method in the :class:`~.ReinforcementSensorManager`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\nimport reverb\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.drivers import py_driver\nfrom tf_agents.networks import sequential\nfrom tf_agents.policies import py_tf_eager_policy, random_tf_policy\nfrom tf_agents.replay_buffers import reverb_replay_buffer, reverb_utils\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.utils import common\n\nnum_iterations = 10000\n\ninitial_collect_steps = 100\ncollect_steps_per_iteration = 1\nreplay_buffer_max_length = 100000\n\nbatch_size = 64\nlearning_rate = 1e-4\nlog_interval = 500\n\nnum_eval_episodes = 10\neval_interval = 1000\nfc_layer_params = (100, 50)\n\n# ---- Optional ----\nmax_train_reward = 250\n\nhyper_parameters = {'num_iterations': num_iterations,\n                    'initial_collect_steps': initial_collect_steps,\n                    'collect_steps_per_iteration': collect_steps_per_iteration,\n                    'replay_buffer_max_length': replay_buffer_max_length,\n                    'batch_size': batch_size,\n                    'learning_rate': learning_rate,\n                    'log_interval': log_interval,\n                    'num_eval_episodes': num_eval_episodes,\n                    'eval_interval': eval_interval,\n                    'fc_layer_params': fc_layer_params,\n                    'max_train_reward': max_train_reward}\n\nreinforcementsensormanager.train(hyper_parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the sensor managers\nThe :func:`choose_actions` function requires a time step and a tracks list as inputs.\n\nFor both sensor management methods, the chosen actions are added to the sensor and measurements made. Tracks which\nhave been observed by the sensor are updated and those that haven\u2019t are predicted forward. These states are appended\nto the tracks list.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run reinforcement learning sensor manager\nTo be able to exploit the policy generated by the reinforcement sensor manager, it  must be passed appropriate\n'timesteps'.\nThese are distinct from the timesteps in Stone Soup, and is of the form time_step_spec from tf-agents.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n\nsensor_history_A = dict()\n\ntimesteps = []\nfor state in truths[0]:\n    timesteps.append(state.timestamp)\n\ntf_timestep = reinforcementsensormanager.test_env.reset()\nreinforcementsensormanager.env.reset()\nfor timestep in timesteps[1:]:\n\n    # Generate chosen configuration\n    # i.e. {a}k\n    # Need to make our own \"timestamp\" that matches tensorflow time_step_spec\n    observation = []\n    uncertainty = []\n    for target in tracksA:\n        x_target = target.state.state_vector[0] - sensorA.position[0]\n        y_target = target.state.state_vector[2] - sensorA.position[1]\n        bearing_target = mod_bearing(np.arctan2(y_target, x_target))\n        uncertainty.append(np.trace(target.covar))\n\n        # observation.append(np.degrees(bearing_target))\n        observation.append(np.trace(target.covar))\n\n    observation = np.array(uncertainty, dtype=np.float32)\n    # observation = np.array(observation, dtype=np.float32)\n\n    chosen_actions = reinforcementsensormanager.choose_actions(tracksA, [sensorA], tf_timestep)\n\n    # Create empty dictionary for measurements\n    measurementsA = []\n\n    for chosen_action in chosen_actions:\n        # chosen_action is a pair of {sensor, action}\n        for sensor, actions in chosen_action.items():\n            sensor.add_actions(list(chain.from_iterable(actions)))\n\n    sensorA.act(timestep)\n\n    # Store sensor history for plotting\n    sensor_history_A[timestep] = copy.copy(sensorA)\n\n    # Observe this ground truth\n    # i.e. {z}k\n    measurements = sensorA.measure(OrderedSet(truth[timestep] for truth in truths), noise=True)\n    measurementsA.extend(measurements)\n\n    hypotheses = data_associator.associate(tracksA,\n                                           measurementsA,\n                                           timestep)\n    for track in tracksA:\n        hypothesis = hypotheses[track]\n        if hypothesis.measurement:\n            post = updater.update(hypothesis)\n            track.append(post)\n        else:  # When data associator says no detections are good enough, we'll keep the prediction\n            track.append(hypothesis.prediction)\n\n    # Propagate environment\n    action_step = reinforcementsensormanager.agent.policy.action(tf_timestep)\n    tf_timestep = reinforcementsensormanager.test_env.step(action_step.action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot ground truths, tracks and uncertainty ellipses for each target.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\nfrom stonesoup.functions import pol2cart\n\nplotterA = AnimatedPlotterly(timesteps, tail_length=1, sim_duration=10)\nplotterA.plot_sensors(sensorA)\nplotterA.plot_ground_truths(truths, [0, 2])\nplotterA.plot_tracks(tracksA, [0, 2], uncertainty=True, plot_history=False)\n\n\ndef plot_sensor_fov(fig, sensor_history):\n    # Plot sensor field of view\n    trace_base = len(fig.data)\n    fig.add_trace(go.Scatter(mode='lines',\n                             line=go.scatter.Line(color='black',\n                                                  dash='dash')))\n\n    for frame in fig.frames:\n        traces_ = list(frame.traces)\n        data_ = list(frame.data)\n        x = [0, 0]\n        y = [0, 0]\n        timestring = frame.name\n        timestamp = datetime.strptime(timestring, \"%Y-%m-%d %H:%M:%S\")\n\n        if timestamp in sensor_history:\n            sensor = sensor_history[timestamp]\n            for i, fov_side in enumerate((-1, 1)):\n                range_ = min(getattr(sensor, 'max_range', np.inf), 100)\n                x[i], y[i] = pol2cart(range_,\n                                      sensor.dwell_centre[0, 0]\n                                      + sensor.fov_angle / 2 * fov_side) \\\n                             + sensor.position[[0, 1], 0]\n        else:\n            continue\n\n        data_.append(go.Scatter(x=[x[0], sensor.position[0], x[1]],\n                                y=[y[0], sensor.position[1], y[1]],\n                                mode=\"lines\",\n                                line=go.scatter.Line(color='black',\n                                                     dash='dash'),\n                                showlegend=False))\n        traces_.append(trace_base)\n        frame.traces = traces_\n        frame.data = data_\n\n\nplot_sensor_fov(plotterA.fig, sensor_history_A)\nplotterA.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run brute force sensor manager\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sensor_history_B = dict()\nfor timestep in timesteps[1:]:\n\n    # Generate chosen configuration\n    # i.e. {a}k\n    chosen_actions = bruteforcesensormanager.choose_actions(tracksB, timestep)\n\n    # Create empty dictionary for measurements\n    measurementsB = set()\n\n    for chosen_action in chosen_actions:\n        for sensor, actions in chosen_action.items():\n            sensor.add_actions(actions)\n\n    sensorB.act(timestep)\n\n    # Store sensor history for plotting\n    sensor_history_B[timestep] = copy.copy(sensorB)\n\n    # Observe this ground truth\n    # i.e. {z}k\n    measurementsB |= sensorB.measure(OrderedSet(truth[timestep] for truth in truths), noise=True)\n\n    hypotheses = data_associator.associate(tracksB,\n                                           measurementsB,\n                                           timestep)\n    for track in tracksB:\n        hypothesis = hypotheses[track]\n        if hypothesis.measurement:\n            post = updater.update(hypothesis)\n            track.append(post)\n        else:  # When data associator says no detections are good enough, we'll keep the prediction\n            track.append(hypothesis.prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot ground truths, tracks and uncertainty ellipses for each target.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotterB = AnimatedPlotterly(timesteps, tail_length=1, sim_duration=10)\nplotterB.plot_sensors(sensorB)\nplotterB.plot_ground_truths(truths, [0, 2])\nplotterB.plot_tracks(tracksB, [0, 2], uncertainty=True, plot_history=False)\nplot_sensor_fov(plotterB.fig, sensor_history_B)\nplotterB.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With a properly trained policy, the :class:`~.ReinforcementSensorManager` performs almost as well as the\n:class:`~.BruteForceSensorManager`. Also, once the policy has been learnt, the time taken to run the\ntracking loop is far smaller for the :class:`~.ReinforcementSensorManager` than for the\n:class:`~.BruteForceSensorManager`, which must re-calculate the best actions each time it is run.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics\nMetrics can be used to compare how well different sensor management techniques are working.\nFull explanations of the OSPA\nand SIAP metrics can be found in the :doc:`Metrics Example <Metrics>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.metricgenerator.ospametric import OSPAMetric\n\nospa_generatorA = OSPAMetric(c=40, p=1,\n                             generator_name='ReinforcementSensorManager',\n                             tracks_key='tracksA',\n                             truths_key='truths')\n\nospa_generatorB = OSPAMetric(c=40, p=1,\n                             generator_name='BruteForceSensorManager',\n                             tracks_key='tracksB',\n                             truths_key='truths')\n\nfrom stonesoup.metricgenerator.tracktotruthmetrics import SIAPMetrics\nfrom stonesoup.measures import Euclidean\n\nsiap_generatorA = SIAPMetrics(position_measure=Euclidean((0, 2)),\n                              velocity_measure=Euclidean((1, 3)),\n                              generator_name='ReinforcementSensorManager',\n                              tracks_key='tracksA',\n                              truths_key='truths')\n\nsiap_generatorB = SIAPMetrics(position_measure=Euclidean((0, 2)),\n                              velocity_measure=Euclidean((1, 3)),\n                              generator_name='BruteForceSensorManager',\n                              tracks_key='tracksB',\n                              truths_key='truths')\n\nfrom stonesoup.dataassociator.tracktotrack import TrackToTruth\n\nassociator = TrackToTruth(association_threshold=30)\n\nfrom stonesoup.metricgenerator.uncertaintymetric import SumofCovarianceNormsMetric\n\nuncertainty_generatorA = SumofCovarianceNormsMetric(generator_name='ReinforcementSensorManager',\n                                                    tracks_key='tracksA')\n\nuncertainty_generatorB = SumofCovarianceNormsMetric(generator_name='BruteForceSensorManager',\n                                                    tracks_key='tracksB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate a metrics manager.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.metricgenerator.manager import MultiManager\n\nmetric_manager = MultiManager([ospa_generatorA,\n                               ospa_generatorB,\n                               siap_generatorA,\n                               siap_generatorB,\n                               uncertainty_generatorA,\n                               uncertainty_generatorB],\n                              associator=associator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each time step, data is added to the metric manager on truths and tracks. The metrics themselves can then be\ngenerated from the metric manager.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metric_manager.add_data({'truths': truths, 'tracksA': tracksA, 'tracksB': tracksB})\n\nmetrics = metric_manager.generate_metrics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OSPA metric\nFirst we look at the OSPA metric. This is plotted over time for each sensor manager method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.plotter import MetricPlotter\n\nfig = MetricPlotter()\nfig.plot_metrics(metrics, metric_names=['OSPA distances'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :class:`~.BruteForceSensorManager` generally results in a smaller OSPA distance\nthan the observations of the :class:`~.ReinforcementSensorManager`, reflecting the better tracking performance\nseen in the tracking plots. At some times, the OSPA distance for the :class:`~.ReinforcementSensorManager` is slightly\nlower than for the :class:`~.BruteForceSensorManager`. While it is intuitive to think that the brute force algorithm\nwould always perform better, the brute force algorithm will pick the target that is most uncertain, and the\nreinforcement algorithm may pick another target that happens to reduce OSPA distance more.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SIAP metrics\nNext we look at SIAP metrics. We are only interested in the positional accuracy (PA) and velocity accuracy (VA).\nThese metrics can be plotted to show how they change over time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig2 = MetricPlotter()\nfig2.plot_metrics(metrics, metric_names=['SIAP Position Accuracy at times',\n                                         'SIAP Velocity Accuracy at times'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to the OSPA distances the :class:`~.BruteForceSensorManager`\ngenerally results in both a better positional accuracy and velocity accuracy than the observations\nof the :class:`~.ReinforcementSensorManager`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Uncertainty metric\nFinally we look at the uncertainty metric which computes the sum of covariance matrix norms of each state at each\ntime step. This is plotted over time for each sensor manager method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig3 = MetricPlotter()\nfig3.plot_metrics(metrics, metric_names=['Sum of Covariance Norms Metric'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This metric shows that the uncertainty in the tracks generated by the :class:`~.ReinforcementSensorManager` is a\nlittle higher than for those generated by the :class:`~.BruteForceSensorManager`. This is also reflected by the\nuncertainty ellipses in the initial plots of tracks and truths.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n.. [#] *https://www.tensorflow.org/install/pip#windows-wsl2*\n.. [#] *https://github.com/tensorflow/agents/blob/master/docs/tutorials/2_environments_tutorial.ipynb*\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}