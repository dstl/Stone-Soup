{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 4 - Reinforcement Learning Sensor Management\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This tutorial introduces using a Deep Q Network (DQN) reinforcement learning (RL) sensor management algorithm in\nStone Soup.\n\nThis is compared to the performance of a brute force algorithm using the same metrics as in previous tutorials.\n\nThis example is similar to previous examples, simulating 3 targets and a :class:`~.RadarRotatingBearingRange` sensor,\nwhich can be actioned to point in different directions.\n\nTensorflow-agents is used as the reinforcement learning framework. This currently only works on Linux based OSes, or\nvia Windows Subsystem for Linux (WSL). See Tensorflow instructions for creating environments (with GPU support if\napplicable) [#]_.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sensor Management example\nSetup\n^^^^^\n\nFirst a simulation must be set up using components from Stone Soup. For this the following imports are required.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport random\nfrom datetime import datetime, timedelta\n\nstart_time = datetime.now()\n\nfrom stonesoup.models.transition.linear import CombinedLinearGaussianTransitionModel, ConstantVelocity\nfrom stonesoup.types.groundtruth import GroundTruthPath, GroundTruthState"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate ground truths\n\nFollowing the methods from previous Stone Soup tutorials, generate a series of combined linear Gaussian transition\nmodels and generate ground truths. Each ground truth is offset in the y-direction by 10.\n\nThe number of targets in this simulation is defined by `ntruths` - here there are 3 targets travelling in different\ndirections. The time the simulation is observed for is defined by `time_max`.\n\nWe can fix our random number generator in order to probe a particular example repeatedly. To produce random examples,\ncomment out the next two lines.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(1990)\nrandom.seed(1990)\n\n# Generate transition model\n# i.e. fk(xk|xk-1)\ntransition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(0.005),\n                                                          ConstantVelocity(0.005)])\n\nyps = range(0, 100, 10)  # y value for prior state\ntruths = []\nntruths = 3  # number of ground truths in simulation\ntime_max = 50  # timestamps the simulation is observed over\n\nxdirection = 1\nydirection = 1\n\n# Generate ground truths\nfor j in range(0, ntruths):\n    truth = GroundTruthPath([GroundTruthState([0, xdirection, yps[j], ydirection], timestamp=start_time)],\n                            id=f\"id{j}\")\n\n    for k in range(1, time_max):\n        truth.append(\n            GroundTruthState(transition_model.function(truth[k - 1], noise=True, time_interval=timedelta(seconds=1)),\n                             timestamp=start_time + timedelta(seconds=k)))\n    truths.append(truth)\n\n    # alternate directions when initiating tracks\n    xdirection *= -1\n    if j % 2 == 0:\n        ydirection *= -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the ground truths. This is done using the :class:`~.Plotterly` class from Stone Soup.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.plotter import Plotterly\n\n# Stonesoup plotter requires sets not lists\ntruths_set = set(truths)\n\nplotter = Plotterly()\nplotter.plot_ground_truths(truths_set, [0, 2])\nplotter.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create sensors\nCreate a sensor for each sensor management algorithm. This tutorial uses the\n:class:`~.RadarRotatingBearingRange` sensor. This sensor is an :class:`~.Actionable` so\nis capable of returning the actions it can possibly\ntake at a given time step and can also be given an action to take before taking\nmeasurements.\nSee the Creating an Actionable Sensor Example for a more detailed explanation of actionable sensors.\n\nThe :class:`~.RadarRotatingBearingRange` has a dwell centre which is an :class:`~.ActionableProperty`\nso in this case the action is changing the dwell centre to point in a specific direction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.types.state import StateVector\nfrom stonesoup.sensor.radar.radar import RadarRotatingBearingRange\n\nsensorA = RadarRotatingBearingRange(\n    position_mapping=(0, 2),\n    noise_covar=np.array([[np.radians(0.5) ** 2, 0],\n                          [0, 1 ** 2]]),\n    ndim_state=4,\n    position=np.array([[10], [0]]),\n    rpm=60,\n    fov_angle=np.radians(45),\n    dwell_centre=StateVector([0.0]),\n    max_range=np.inf\n)\nsensorA.timestamp = start_time\n\nsensorB = RadarRotatingBearingRange(\n    position_mapping=(0, 2),\n    noise_covar=np.array([[np.radians(0.5) ** 2, 0],\n                          [0, 1 ** 2]]),\n    ndim_state=4,\n    position=np.array([[10], [0]]),\n    rpm=60,\n    fov_angle=np.radians(45),\n    dwell_centre=StateVector([0.0]),\n    max_range=np.inf\n)\nsensorB.timestamp = start_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the Kalman predictor and updater\n\nConstruct a predictor and updater using the :class:`~.KalmanPredictor` and :class:`~.ExtendedKalmanUpdater`\ncomponents from Stone Soup. The :class:`~.ExtendedKalmanUpdater` is used because it can be used for both linear\nand nonlinear measurement models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.predictor.kalman import KalmanPredictor\npredictor = KalmanPredictor(transition_model)\n\nfrom stonesoup.updater.kalman import ExtendedKalmanUpdater\nupdater = ExtendedKalmanUpdater(measurement_model=None)\n# measurement model is added to detections by the sensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the Kalman filters\n\nFirst create `ntruths` priors which estimate the targets\u2019 initial states, one for each target. In this example\neach prior is offset by 0.5 in the y direction meaning the position of the track is initially not very accurate. The\nvelocity is also systematically offset by +0.5 in both the x and y directions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.types.state import GaussianState\n\npriors = []\nxdirection = 1.2\nydirection = 1.2\nfor j in range(0, ntruths):\n    priors.append(GaussianState([[0], [xdirection], [yps[j]+0.1], [ydirection]],\n                                np.diag([0.5, 0.5, 0.5, 0.5]+np.random.normal(0,5e-4,4)),\n                                timestamp=start_time))\n    xdirection *= -1\n    if j % 2 == 0:\n        ydirection *= -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialise the tracks by creating an empty list and appending the priors generated. This needs to be done separately\nfor both sensor manager methods as they will generate different sets of tracks.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.types.track import Track\n\n# Initialise tracks from the RandomSensorManager\ntracksA = []\nfor j, prior in enumerate(priors):\n    tracksA.append(Track([prior]))\n\ntracksB = []\nfor j, prior in enumerate(priors):\n    tracksB.append(Track([prior]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reward function\n\nA reward function is used to quantify the benefit of sensors taking a particular action or set of actions.\nThis can be crafted specifically for an example in order to achieve a particular objective. The function used in\nthis example is quite generic but could be substituted for any callable function which returns a numeric\nvalue that the sensor manager can maximise.\n\nThe :class:`~.UncertaintyRewardFunction` calculates the uncertainty reduction by computing the difference between the\ncovariance matrix norms of the\nprediction, and the posterior assuming a predicted measurement corresponding to that prediction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.hypothesiser.distance import DistanceHypothesiser\nfrom stonesoup.measures import Mahalanobis\nhypothesiser = DistanceHypothesiser(predictor, updater, measure=Mahalanobis(), missed_distance=5)\n\nfrom stonesoup.dataassociator.neighbour import GNNWith2DAssignment\ndata_associator = GNNWith2DAssignment(hypothesiser)\n\nfrom stonesoup.sensormanager.reward import UncertaintyRewardFunction\nreward_function = UncertaintyRewardFunction(predictor=predictor, updater=updater)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reinforcement Learning\n\nReinforcement learning is when an intelligent agents make decisions to maximise a cumulative reward. First, the agent\nmust train in an environment to create a policy, which determines which actions to take. During training, the agent\nmakes decisions and receives rewards, which is uses to optimise the policy.\n\n<img src=\"https://stonesoup.rtfd.io/en/latest/_static/rl_training.png\" width=\"800\" alt=\"Illustration of sequential actions and measurements\">\n\nOnce training has completed, the policy can be exploited to gain rewards.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Design Environment\n\nAn environment is needed for the RL agent to learn in. There are resources online for how to design these [#]_.\n\nFor this tutorial, a pre-designed environment has been created for you to go through.\nIn this example, the action space is equal to the number of targets in the simulation, so at each time step, the\nsensor can look at one target.\nThe :class:`~.UncertaintyRewardFunction` to calculate the reward obtained for each step in the environment.\nThe trace of the covariances for each object is used as the observation for the agent to learn from.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.sensormanager.reinforcement import BaseEnvironment\nimport numpy as np\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.environments import utils\nfrom stonesoup.sensor.action.dwell_action import DwellActionsGenerator\nfrom stonesoup.functions import mod_bearing\nimport copy\nfrom ordered_set import OrderedSet\n\n\nclass StoneSoupEnv(BaseEnvironment):\n    \"\"\"Example reinforcement learning environment. Environments must contain __init__, _reset,\n    _step, and generate_action methods\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # Action size is number of targets\n        self._action_spec = array_spec.BoundedArraySpec(\n            shape=(), dtype=np.int32, minimum=0, maximum=ntruths-1, name='action')\n        # Observation size is also number of targets\n        self.obs_size = ntruths\n        self._observation_spec = array_spec.BoundedArraySpec(\n            shape=(self.obs_size,), dtype=np.float32, name='observation')\n        self._episode_ended = False\n        self.max_episode_length = time_max\n        self.current_step = 0\n        self.start_time = start_time\n        # Use deepcopy to prevent the original sensor/tracks being changed\n        self.sensor = copy.deepcopy(sensorA)\n        self.sensor.timestamp = start_time\n        self.tracks = copy.deepcopy(tracksA)\n\n    def _reset(self):\n        \"\"\"Restarts the environment from the first step, resets the initial state\n        and observation values, and returns an initial observation\n        \"\"\"\n        self._episode_ended = False\n        self.current_step = 0\n        self.sensor = copy.deepcopy(sensorA)\n        self.sensor.timestamp = start_time\n        self.tracks = copy.deepcopy(tracksA)\n        return ts.restart(np.zeros((self.obs_size,), dtype=np.float32))\n\n    def _step(self, action):\n        \"\"\"Apply action and take one step through environment, and return new time_step.\n        \"\"\"\n\n        reward = 0\n        if self._episode_ended:\n            # The last action ended the episode. Ignore the current action and start\n            # a new episode.\n            return self.reset()\n\n        uncertainty = []\n        for i, target in enumerate(self.tracks):\n            # Calculate the bearing of the chosen target from the sensor\n            if i == action:\n                x_target = target.state.state_vector[0]-self.sensor.position[0]\n                y_target = target.state.state_vector[2]-self.sensor.position[1]\n                bearing_target = mod_bearing(np.arctan2(y_target, x_target))\n\n            uncertainty.append(np.trace(target.covar))\n\n        current_timestep = self.start_time + timedelta(seconds=self.current_step)\n        next_timestep = self.start_time + timedelta(seconds=self.current_step+1)\n\n        # Create action generator which contains possible actions\n        action_generator = DwellActionsGenerator(self.sensor,\n                                                 attribute='dwell_centre',\n                                                 start_time=current_timestep,\n                                                 end_time=next_timestep)\n\n        # Action the environment's sensor to point towards the chosen target\n        current_action = [action_generator.action_from_value(bearing_target)]\n        config = ({self.sensor: current_action})\n        reward += reward_function(config, self.tracks, next_timestep)\n\n        self.sensor.add_actions(current_action)\n        self.sensor.act(next_timestep)\n\n        # Calculate a measurement from the sensor\n        measurement = set()\n        measurement |= self.sensor.measure(OrderedSet(truth[current_timestep] for truth in truths), noise=True)\n        hypotheses = data_associator.associate(self.tracks,\n                                               measurement,\n                                               current_timestep)\n\n        for track in self.tracks:\n            hypothesis = hypotheses[track]\n            if hypothesis.measurement:\n                post = updater.update(hypothesis)\n                track.append(post)\n            else:  # When data associator says no detections are good enough, we'll keep the prediction\n                track.append(hypothesis.prediction)\n\n        # Set the observation as the prior uncertainty of each target\n        observation = np.array(uncertainty, dtype=np.float32)\n\n        self.current_step += 1\n\n        if self.current_step >= self.max_episode_length-1:\n            self._episode_ended = True\n            return ts.termination(observation, reward)\n        else:\n            return ts.transition(observation, reward=reward, discount=1.0)\n\n    @staticmethod\n    def generate_action(action, tracks, sensor):\n        \"\"\"This method is used to convert a tf-agents action into a Stone Soup action\"\"\"\n        for i, target in enumerate(tracks):\n            if i == action:\n                x_target = target.state.state_vector[0]-sensor.position[0]\n                y_target = target.state.state_vector[2]-sensor.position[1]\n                action_bearing = mod_bearing(np.arctan2(y_target, x_target))\n\n        action_generators = DwellActionsGenerator(sensor,\n                                                  attribute='dwell_centre',\n                                                  start_time=sensor.timestamp,\n                                                  end_time=sensor.timestamp+timedelta(seconds=1))\n\n        current_action = [action_generators.action_from_value(action_bearing)]\n        return current_action\n\n# Validate the environment to ensure that the environment returns the expected specs\ntrain_env = StoneSoupEnv()\nutils.validate_py_environment(train_env, episodes=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Sensor Managers\n\nWe initiate our reinforcement learning sensor manager with the environment we have designed\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.sensormanager.reinforcement import ReinforcementSensorManager\nreinforcementsensormanager = ReinforcementSensorManager({sensorA}, env=StoneSoupEnv())\n\nfrom stonesoup.sensormanager import BruteForceSensorManager\nbruteforcesensormanager = BruteForceSensorManager({sensorB}, reward_function=reward_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train RL agent\n\nTo generate a policy, we need to train the reinforcement learning agent using the environment we created above.\nSome hyperparameters are created that the agent uses to train with.\n\nTo train the agent, the hyperparameters are passed to the train method in the :class:`~.ReinforcementSensorManager`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_iterations = 10000\n\ninitial_collect_steps = 100\ncollect_steps_per_iteration = 1\nreplay_buffer_max_length = 100000\n\nbatch_size = 64\nlearning_rate = 1e-4\nlog_interval = 500\n\nnum_eval_episodes = 10\neval_interval = 1000\nfc_layer_params = (100, 50)\n\n# ---- Optional ----\nmax_train_reward = 250\n\nhyper_parameters = {'num_iterations': num_iterations,\n                    'initial_collect_steps': initial_collect_steps,\n                    'collect_steps_per_iteration': collect_steps_per_iteration,\n                    'replay_buffer_max_length': replay_buffer_max_length,\n                    'batch_size': batch_size,\n                    'learning_rate': learning_rate,\n                    'log_interval': log_interval,\n                    'num_eval_episodes': num_eval_episodes,\n                    'eval_interval': eval_interval,\n                    'fc_layer_params': fc_layer_params,\n                    'max_train_reward': max_train_reward}\n\nreinforcementsensormanager.train(hyper_parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the sensor managers\n\nTo be able to exploit the policy generated by the reinforcement sensor manager, it  must be passed appropriate\n'timesteps'.\nThese are distinct from the timesteps in Stonesoup, and is of the form time_step_spec from tf-agents.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n\ntimesteps = []\nfor state in truths[0]:\n    timesteps.append(state.timestamp)\n\ntf_timestep = reinforcementsensormanager.test_env.reset()\nreinforcementsensormanager.env.reset()\nfor timestep in timesteps[1:]:\n\n    # Generate chosen configuration\n    # i.e. {a}k\n    # Need to make our own \"timestamp\" that matches tensorflow time_step_spec\n    observation = []\n    uncertainty = []\n    for target in tracksA:\n        x_target = target.state.state_vector[0]-sensorA.position[0]\n        y_target = target.state.state_vector[2]-sensorA.position[1]\n        bearing_target = mod_bearing(np.arctan2(y_target, x_target))\n        uncertainty.append(np.trace(target.covar))\n\n        # observation.append(np.degrees(bearing_target))\n        observation.append(np.trace(target.covar))\n\n    observation = np.array(uncertainty, dtype=np.float32)\n    # observation = np.array(observation, dtype=np.float32)\n\n    chosen_actions = reinforcementsensormanager.choose_actions(tracksA, [sensorA], tf_timestep)\n\n    # Create empty dictionary for measurements\n    measurementsA = []\n\n    for chosen_action in chosen_actions:\n        # chosen_action is a pair of {sensor, action}\n        for sensor, actions in chosen_action.items():\n            sensor.add_actions(list(chain.from_iterable(actions)))\n\n    sensorA.act(timestep)\n\n    # Observe this ground truth\n    # i.e. {z}k\n    measurements = sensorA.measure(OrderedSet(truth[timestep] for truth in truths), noise=True)\n    measurementsA.extend(measurements)\n\n    hypotheses = data_associator.associate(tracksA,\n                                           measurementsA,\n                                           timestep)\n    for track in tracksA:\n        hypothesis = hypotheses[track]\n        if hypothesis.measurement:\n            post = updater.update(hypothesis)\n            track.append(post)\n        else:  # When data associator says no detections are good enough, we'll keep the prediction\n            track.append(hypothesis.prediction)\n\n    # Propagate environment\n    action_step = reinforcementsensormanager.agent.policy.action(tf_timestep)\n    tf_timestep = reinforcementsensormanager.test_env.step(action_step.action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot ground truths, tracks and uncertainty ellipses for each target.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotterA = Plotterly()\nplotterA.plot_sensors(sensorA)\nplotterA.plot_ground_truths(truths_set, [0, 2])\nplotterA.plot_tracks(set(tracksA), [0, 2], uncertainty=True)\nplotterA.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run brute force sensor manager\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for timestep in timesteps[1:]:\n\n    # Generate chosen configuration\n    # i.e. {a}k\n    chosen_actions = bruteforcesensormanager.choose_actions(tracksB, timestep)\n\n    # Create empty dictionary for measurements\n    measurementsB = set()\n\n    for chosen_action in chosen_actions:\n        for sensor, actions in chosen_action.items():\n            sensor.add_actions(actions)\n\n    sensorB.act(timestep)\n\n    # Observe this ground truth\n    # i.e. {z}k\n    measurementsB |= sensorB.measure(OrderedSet(truth[timestep] for truth in truths), noise=True)\n\n    hypotheses = data_associator.associate(tracksB,\n                                           measurementsB,\n                                           timestep)\n    for track in tracksB:\n        hypothesis = hypotheses[track]\n        if hypothesis.measurement:\n            post = updater.update(hypothesis)\n            track.append(post)\n        else:  # When data associator says no detections are good enough, we'll keep the prediction\n            track.append(hypothesis.prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot ground truths, tracks and uncertainty ellipses for each target.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotterB = Plotterly()\nplotterB.plot_sensors(sensorA)\nplotterB.plot_ground_truths(truths_set, [0, 2])\nplotterB.plot_tracks(set(tracksB), [0, 2], uncertainty=True)\nplotterB.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With a properly trained policy, the :class:`~.ReinforcementSensorManager` performs almost as well as the\n:class:`~.BruteForceSensorManager`. Also, once the policy has been learnt, the time taken to run the\ntracking loop is far smaller for the :class:`~.ReinforcementSensorManager` than for the\n:class:`~.BruteForceSensorManager`, which must re-calculate the best actions each time it is run.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Metrics\n\nMetrics can be used to compare how well different sensor management techniques are working.\nFull explanations of the OSPA\nand SIAP metrics can be found in the Metrics Example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.metricgenerator.ospametric import OSPAMetric\nospa_generatorA = OSPAMetric(c=40, p=1,\n                             generator_name='ReinforcementSensorManager',\n                             tracks_key='tracksA',\n                             truths_key='truths')\n\nospa_generatorB = OSPAMetric(c=40, p=1,\n                             generator_name='BruteForceSensorManager',\n                             tracks_key='tracksB',\n                             truths_key='truths')\n\nfrom stonesoup.metricgenerator.tracktotruthmetrics import SIAPMetrics\nfrom stonesoup.measures import Euclidean\nsiap_generatorA = SIAPMetrics(position_measure=Euclidean((0, 2)),\n                              velocity_measure=Euclidean((1, 3)),\n                              generator_name='Reinforcement',\n                              tracks_key='tracksA',\n                              truths_key='truths')\n\nsiap_generatorB = SIAPMetrics(position_measure=Euclidean((0, 2)),\n                              velocity_measure=Euclidean((1, 3)),\n                              generator_name='BruteForceSensorManager',\n                              tracks_key='tracksB',\n                              truths_key='truths')\n\nfrom stonesoup.dataassociator.tracktotrack import TrackToTruth\nassociator = TrackToTruth(association_threshold=30)\n\nfrom stonesoup.metricgenerator.uncertaintymetric import SumofCovarianceNormsMetric\nuncertainty_generatorA = SumofCovarianceNormsMetric(generator_name='ReinforcementSensorManager',\n                                                    tracks_key='tracksA')\n\nuncertainty_generatorB = SumofCovarianceNormsMetric(generator_name='BruteForceSensorManager',\n                                                    tracks_key='tracksB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate a metrics manager.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.metricgenerator.manager import MultiManager\n\nmetric_manager = MultiManager([ospa_generatorA,\n                               ospa_generatorB,\n                               siap_generatorA,\n                               siap_generatorB,\n                               uncertainty_generatorA,\n                               uncertainty_generatorB],\n                               associator=associator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each time step, data is added to the metric manager on truths and tracks. The metrics themselves can then be\ngenerated from the metric manager.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metric_manager.add_data({'truths': truths, 'tracksA': tracksA, 'tracksB': tracksB})\n\nmetrics = metric_manager.generate_metrics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OSPA metric\n\nFirst we look at the OSPA metric. This is plotted over time for each sensor manager method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.plotter import MetricPlotter\n\nfig = MetricPlotter()\nfig.plot_metrics(metrics, metric_names=['OSPA distances'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :class:`~.BruteForceSensorManager` generally results in a smaller OSPA distance\nthan the observations of the :class:`~.ReinforcementSensorManager`, reflecting the better tracking performance\nseen in the tracking plots.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SIAP metrics\n\nNext we look at SIAP metrics. We are only interested in the positional accuracy (PA) and velocity accuracy (VA).\nThese metrics can be plotted to show how they change over time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig2 = MetricPlotter()\nfig2.plot_metrics(metrics, metric_names=['SIAP Position Accuracy at times',\n                                         'SIAP Velocity Accuracy at times'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to the OSPA distances the :class:`~.BruteForceSensorManager`\ngenerally results in both a better positional accuracy and velocity accuracy than the observations\nof the :class:`~.ReinforcementSensorManager`.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Uncertainty metric\n\nFinally we look at the uncertainty metric which computes the sum of covariance matrix norms of each state at each\ntime step. This is plotted over time for each sensor manager method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig3 = MetricPlotter()\nfig3.plot_metrics(metrics, metric_names=['Sum of Covariance Norms Metric'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This metric shows that the uncertainty in the tracks generated by the :class:`~.ReinforcementSensorManager` is a\nlittle higher than for those generated by the :class:`~.BruteForceSensorManager`. This is also reflected by the\nuncertainty ellipses in the initial plots of tracks and truths.\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### References\n\n.. [#] *https://www.tensorflow.org/install/pip#windows-wsl2*\n.. [#] *https://github.com/tensorflow/agents/blob/master/docs/tutorials/2_environments_tutorial.ipynb*\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}